{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NpcLwGS0x0bq",
        "outputId": "715c347d-b8cd-4e05-d708-5fca4c531ec1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Colab Notebooks/gpt-mini-background\n",
            "'background execution test'\t        gpt-mini-background.ipynb\n",
            "'background execution test 2'\t        gpt_mini_epoch_1_iter_100000.pth\n",
            " background_log.txt\t\t        gpt_mini_epoch_1_iter_120000.pth\n",
            " background_task.py\t\t        gpt_mini_epoch_1_iter_140000.pth\n",
            "'Copy of gpt-mini-background.ipynb'     gpt_mini_epoch_1_iter_160000.pth\n",
            " gpt_mini_2_epoch_1_iter_10000.pth      gpt_mini_epoch_1_iter_20000.pth\n",
            " gpt_mini_2_epoch_1_iter_20000.pth      gpt_mini_epoch_1_iter_40000.pth\n",
            " gpt_mini_2_epoch_1_iter_30000.pth      gpt_mini_epoch_1_iter_60000.pth\n",
            " gpt_mini_2_epoch_1_iter_40000.pth      gpt_mini_epoch_1_iter_80000.pth\n",
            " gpt_mini_2_epoch_1_iter_50000.pth      gpt_mini_epoch_1.pth\n",
            " gpt_mini_2_epoch_1_iter_60000.pth      gpt_mini_epoch_2_iter_100000.pth\n",
            " gpt_mini_2_epoch_1_iter_70000.pth      gpt_mini_epoch_2_iter_20000.pth\n",
            " gpt_mini_3_epoch_1_iter_30000.pth      gpt_mini_epoch_2_iter_40000.pth\n",
            " gpt_mini_3_epoch_1_iter_60000.pth      gpt_mini_epoch_2_iter_60000.pth\n",
            " gpt_mini_3_epoch_1_iter_90000.pth      gpt_mini_epoch_2_iter_80000.pth\n",
            " gpt_mini_4_epoch_1_iter_30000.pth      nohup.out\n",
            " gpt_mini_4_epoch_1_iter_60000.pth      prepare.ipynb\n",
            " gpt_mini_4_epoch_1_iter_90000.pth      wandb\n",
            " gpt-mini-background-checkpoint-1.pth\n"
          ]
        }
      ],
      "source": [
        "%cd /content/drive/MyDrive/Colab Notebooks/gpt-mini-background\n",
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "usIxm7k0yGNe"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "import torch.optim\n",
        "\n",
        "# hyperparameters\n",
        "batch_size = 64 # how many independent sequences will be processed in parallel\n",
        "seq_len = 128 # the maximum context length for predictions\n",
        "vocab_size = 50258\n",
        "learning_rate = 5e-4\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "n_embd = 256 # same as d_model\n",
        "n_head = 8\n",
        "n_layer = 8\n",
        "dropout = 0.2\n",
        "EPOCHS = 20"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cLv-e-e-x64n"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "\n",
        "class MemmapDataset(Dataset):\n",
        "    def __init__(self, data_dir, seq_len):\n",
        "        self.dataset = np.memmap(data_dir, dtype=np.uint16, mode='r')\n",
        "        self.seq_len = seq_len\n",
        "        self.total_length = (len(self.dataset) - 1) // (seq_len + 1)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.total_length\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        start_idx = idx * (self.seq_len + 1)\n",
        "        end_idx = start_idx + self.seq_len\n",
        "        input_sequence = self.dataset[start_idx:end_idx].astype(np.int64)\n",
        "        target_sequence = self.dataset[start_idx+1:end_idx+1].astype(np.int64)\n",
        "        return torch.tensor(input_sequence, dtype=torch.long), torch.tensor(target_sequence, dtype=torch.long)\n",
        "\n",
        "\n",
        "train_dir = '/content/drive/MyDrive/Colab Notebooks/Transformers/train_new.bin'\n",
        "val_dir = '/content/drive/MyDrive/Colab Notebooks/Transformers/val_new.bin'\n",
        "\n",
        "train_dataset = MemmapDataset(train_dir, seq_len=seq_len)\n",
        "val_dataset = MemmapDataset(val_dir, seq_len=seq_len)\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=8)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=8)\n",
        "# The optimal number of num_workers is found using an iterative approach, and measuring time, in prepare_v3.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "UNV1CaTJySYl"
      },
      "outputs": [],
      "source": [
        "class Head(nn.Module):\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(seq_len, seq_len)))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # inputs: (batch, time-step, channels) == (batch_size, seq_len, d_model)\n",
        "        # outputs: (batch, time-step, head size) == (batch_size, seq_len, d_k)\n",
        "        B, T, C = x.shape\n",
        "        k = self.key(x)   # (B, T, hs)\n",
        "        q = self.query(x) # (B, T, hs)\n",
        "\n",
        "        attention_scores = q @ k.transpose(-2, -1) * k.shape[-1]**-0.5 # (B, T, hs) @ (B, hs, T) -> (B, T, T) *biggest mistake: -0.5*\n",
        "        attention_scores = attention_scores.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
        "        attention_scores = F.softmax(attention_scores, dim=-1)\n",
        "        attention_scores = self.dropout(attention_scores)\n",
        "\n",
        "        v = self.value(x) # (B, T, hs)\n",
        "        out = attention_scores @ v # (B, T, T) @ (B, T, hs) -> (B, T, hs)\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.w_o = nn.Linear(head_size * num_heads, n_embd) # (d_k * h, d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.w_o(out))\n",
        "        return out\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedForward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd) # planning to make myself\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "class GPTLanguageModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(seq_len, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd)\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        tok_emb = self.token_embedding_table(idx) # (B, T, C) == (Batchsize, seqlen, dmodel)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device))\n",
        "        x = tok_emb + pos_emb\n",
        "        x = self.blocks(x) # (B, T, C)\n",
        "        x = self.ln_f(x) # (B, T, C)\n",
        "        logits = self.lm_head(x) # (B, T, vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        for _ in range(max_new_tokens):\n",
        "            idx_cond = idx[:, -seq_len:]\n",
        "            logits, loss = self(idx_cond)\n",
        "            logits = logits[:, -1, :]\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "            idx = torch.cat((idx, idx_next), dim=1)\n",
        "        return idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9xFQcZ0NyVi1",
        "outputId": "1162e003-d52a-419e-ab70-b2e3b306562f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.7/6.7 MB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m26.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m281.6/281.6 kB\u001b[0m \u001b[31m35.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "!pip install wandb -qU\n",
        "import wandb\n",
        "wandb.login(key=\"b30a84eeb0db02eac6ee82b8044c05ec0fed4911\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XeL7QOvVyYzE",
        "outputId": "4d7709bd-5f5e-4ba2-c7ee-e085816a296f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
          ]
        }
      ],
      "source": [
        "model = GPTLanguageModel()\n",
        "model = model.to(device)\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.2, patience=2, verbose=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        },
        "id": "gw2jo-j-yWs4",
        "outputId": "84f5845c-a578-49fa-faac-644dbd9dd838"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33myoshisato\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.0"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/drive/MyDrive/Colab Notebooks/gpt-mini-background/wandb/run-20240523_014911-etaallzv</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/yoshisato/gpt-mini-training-project/runs/etaallzv' target=\"_blank\">distinctive-river-7</a></strong> to <a href='https://wandb.ai/yoshisato/gpt-mini-training-project' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/yoshisato/gpt-mini-training-project' target=\"_blank\">https://wandb.ai/yoshisato/gpt-mini-training-project</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/yoshisato/gpt-mini-training-project/runs/etaallzv' target=\"_blank\">https://wandb.ai/yoshisato/gpt-mini-training-project/runs/etaallzv</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/yoshisato/gpt-mini-training-project/runs/etaallzv?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7ec8a22cd7b0>"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "wandb.init(\n",
        "    project=\"gpt-mini-training-project\",\n",
        "\n",
        "    config={\n",
        "    \"architecture\": \"Decoder-only Transformer\",\n",
        "    \"dataset\": \"OpenWebText\",\n",
        "    \"EPOCHS\": EPOCHS,\n",
        "    \"Iterations per Epoch\": len(train_dataloader),\n",
        "    \"Model Parameters\" : sum(p.numel() for p in model.parameters() if p.requires_grad),\n",
        "    \"learning_rate\": learning_rate,\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "32gi1spxyi1o"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    model.eval()\n",
        "    running_loss = 0.0;\n",
        "    for batch in val_dataloader:\n",
        "        val_input_sequences, val_target_sequences = batch\n",
        "        val_input_sequences = val_input_sequences.to(device)\n",
        "        val_target_sequences = val_target_sequences.to(device)\n",
        "\n",
        "        logits, loss = model(val_input_sequences, val_target_sequences)\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    avg_loss = running_loss / len(val_dataloader)\n",
        "    model.train()\n",
        "    return avg_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "awiM2_7Gydmx",
        "outputId": "c63dfc2e-598a-47d7-84d3-b3916e829683"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1/20, Batch: 2000/136529, Loss: 5.880917208909988, LR: 0.0005\n",
            "Epoch: 1/20, Batch: 4000/136529, Loss: 5.166393566370011, LR: 0.0005\n",
            "Epoch: 1/20, Batch: 6000/136529, Loss: 4.956403430700302, LR: 0.0005\n",
            "Epoch: 1/20, Batch: 8000/136529, Loss: 4.838126055240631, LR: 0.0005\n",
            "Epoch: 1/20, Batch: 10000/136529, Loss: 4.760191371679306, LR: 0.0005\n",
            "Epoch: 1/20, Batch: 10000/136529, Validation Loss: 4.615859305108343\n",
            "Epoch: 1/20, Batch: 12000/136529, Loss: 4.698157769203186, LR: 0.0005\n",
            "Epoch: 1/20, Batch: 14000/136529, Loss: 4.652158542394638, LR: 0.0005\n",
            "Epoch: 1/20, Batch: 16000/136529, Loss: 4.61181666970253, LR: 0.0005\n",
            "Epoch: 1/20, Batch: 18000/136529, Loss: 4.57833806848526, LR: 0.0005\n",
            "Epoch: 1/20, Batch: 20000/136529, Loss: 4.549838432312011, LR: 0.0005\n",
            "Epoch: 1/20, Batch: 20000/136529, Validation Loss: 4.404465201851371\n",
            "Epoch: 1/20, Batch: 22000/136529, Loss: 4.526633403062821, LR: 0.0005\n",
            "Epoch: 1/20, Batch: 24000/136529, Loss: 4.502944415330886, LR: 0.0005\n",
            "Epoch: 1/20, Batch: 26000/136529, Loss: 4.483987631797791, LR: 0.0005\n",
            "Epoch: 1/20, Batch: 28000/136529, Loss: 4.47079768037796, LR: 0.0005\n",
            "Epoch: 1/20, Batch: 30000/136529, Loss: 4.460022549629212, LR: 0.0005\n",
            "Epoch: 1/20, Batch: 30000/136529, Validation Loss: 4.3098393937090895\n",
            "Checkpoint saved at epoch 1 and iteration 30000\n",
            "Epoch: 1/20, Batch: 32000/136529, Loss: 4.446465371847153, LR: 0.0005\n",
            "Epoch: 1/20, Batch: 34000/136529, Loss: 4.435790815114975, LR: 0.0005\n",
            "Epoch: 1/20, Batch: 36000/136529, Loss: 4.422605679273605, LR: 0.0005\n",
            "Epoch: 1/20, Batch: 38000/136529, Loss: 4.414275395154953, LR: 0.0005\n",
            "Epoch: 1/20, Batch: 40000/136529, Loss: 4.407077876091003, LR: 0.0005\n",
            "Epoch: 1/20, Batch: 40000/136529, Validation Loss: 4.257072908894999\n",
            "Epoch: 1/20, Batch: 42000/136529, Loss: 4.397653620958328, LR: 0.0005\n",
            "Epoch: 1/20, Batch: 44000/136529, Loss: 4.391783341407776, LR: 0.0005\n",
            "Epoch: 1/20, Batch: 46000/136529, Loss: 4.383187532424927, LR: 0.0005\n",
            "Epoch: 1/20, Batch: 48000/136529, Loss: 4.376826916456222, LR: 0.0005\n",
            "Epoch: 1/20, Batch: 50000/136529, Loss: 4.3714111535549165, LR: 0.0005\n",
            "Epoch: 1/20, Batch: 50000/136529, Validation Loss: 4.22393120752348\n",
            "Epoch: 1/20, Batch: 52000/136529, Loss: 4.363960096597672, LR: 0.0005\n",
            "Epoch: 1/20, Batch: 54000/136529, Loss: 4.359517374038696, LR: 0.0005\n",
            "Epoch: 1/20, Batch: 56000/136529, Loss: 4.3529882686138155, LR: 0.0005\n",
            "Epoch: 1/20, Batch: 58000/136529, Loss: 4.349375778913498, LR: 0.0005\n",
            "Epoch: 1/20, Batch: 60000/136529, Loss: 4.34574480342865, LR: 0.0005\n",
            "Epoch: 1/20, Batch: 60000/136529, Validation Loss: 4.192743453112516\n",
            "Checkpoint saved at epoch 1 and iteration 60000\n",
            "Epoch: 1/20, Batch: 62000/136529, Loss: 4.33864525604248, LR: 0.0005\n",
            "Epoch: 1/20, Batch: 64000/136529, Loss: 4.335063226222992, LR: 0.0005\n",
            "Epoch: 1/20, Batch: 66000/136529, Loss: 4.330915794849396, LR: 0.0005\n",
            "Epoch: 1/20, Batch: 68000/136529, Loss: 4.3288157572746275, LR: 0.0005\n",
            "Epoch: 1/20, Batch: 70000/136529, Loss: 4.325704684495926, LR: 0.0005\n",
            "Epoch: 1/20, Batch: 70000/136529, Validation Loss: 4.175239081149335\n",
            "Epoch: 1/20, Batch: 72000/136529, Loss: 4.323327250480652, LR: 0.0005\n",
            "Epoch: 1/20, Batch: 74000/136529, Loss: 4.317875238895416, LR: 0.0005\n",
            "Epoch: 1/20, Batch: 76000/136529, Loss: 4.310820427894592, LR: 0.0005\n",
            "Epoch: 1/20, Batch: 78000/136529, Loss: 4.312450865983963, LR: 0.0005\n",
            "Epoch: 1/20, Batch: 80000/136529, Loss: 4.305293887138367, LR: 0.0005\n",
            "Epoch: 1/20, Batch: 80000/136529, Validation Loss: 4.160682658215503\n",
            "Epoch: 1/20, Batch: 82000/136529, Loss: 4.307147759437561, LR: 0.0005\n",
            "Epoch: 1/20, Batch: 84000/136529, Loss: 4.303298649072647, LR: 0.0005\n",
            "Epoch: 1/20, Batch: 86000/136529, Loss: 4.3007782604694365, LR: 0.0005\n",
            "Epoch: 1/20, Batch: 88000/136529, Loss: 4.297444048404693, LR: 0.0005\n",
            "Epoch: 1/20, Batch: 90000/136529, Loss: 4.294886607170105, LR: 0.0005\n",
            "Epoch: 1/20, Batch: 90000/136529, Validation Loss: 4.146816372037767\n",
            "Checkpoint saved at epoch 1 and iteration 90000\n",
            "Epoch: 1/20, Batch: 92000/136529, Loss: 4.289533069610596, LR: 0.0005\n",
            "Epoch: 1/20, Batch: 94000/136529, Loss: 4.2916777377128605, LR: 0.0005\n",
            "Epoch: 1/20, Batch: 96000/136529, Loss: 4.289478301048279, LR: 0.0005\n",
            "Epoch: 1/20, Batch: 98000/136529, Loss: 4.2852213571071625, LR: 0.0005\n",
            "Epoch: 1/20, Batch: 100000/136529, Loss: 4.283181188583374, LR: 0.0005\n",
            "Epoch: 1/20, Batch: 100000/136529, Validation Loss: 4.135703922151686\n",
            "Epoch: 1/20, Batch: 102000/136529, Loss: 4.28493248128891, LR: 0.0005\n",
            "Epoch: 1/20, Batch: 104000/136529, Loss: 4.280037462234497, LR: 0.0005\n",
            "Epoch: 1/20, Batch: 106000/136529, Loss: 4.279554371833801, LR: 0.0005\n",
            "Epoch: 1/20, Batch: 108000/136529, Loss: 4.275678929328919, LR: 0.0005\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-cf44df4ed359>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_sequences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_sequences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset_to_none\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    523\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m             )\n\u001b[0;32m--> 525\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    526\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 267\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    268\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    743\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 744\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    745\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    746\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Training Loop Parameters\n",
        "total_batches = len(train_dataloader)\n",
        "log_interval = 2000\n",
        "val_interval = 10000\n",
        "checkpoint_interval = 30000\n",
        "current_step = 0\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for i, batch in enumerate(train_dataloader):\n",
        "        input_sequences, target_sequences = batch\n",
        "        input_sequences = input_sequences.to(device)\n",
        "        target_sequences = target_sequences.to(device)\n",
        "\n",
        "        logits, loss = model(input_sequences, target_sequences)\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        # Logging training progress\n",
        "        if (current_step+1) % log_interval == 0:\n",
        "            avg_loss = running_loss / log_interval\n",
        "            current_lr = optimizer.param_groups[0]['lr']\n",
        "            print(f\"Epoch: {epoch+1}/{EPOCHS}, Batch: {i+1}/{total_batches}, Loss: {avg_loss}, LR: {current_lr}\")\n",
        "            wandb.log({\"Training Loss\": avg_loss, \"Learning Rate\": current_lr})\n",
        "            running_loss = 0.0\n",
        "\n",
        "        # Validation and learning rate adjustment\n",
        "        if (current_step+1) % val_interval == 0:\n",
        "            val_loss = estimate_loss()\n",
        "            print(f\"Epoch: {epoch+1}/{EPOCHS}, Batch: {i+1}/{total_batches}, Validation Loss: {val_loss}\")\n",
        "            wandb.log({\"Validation Loss\": val_loss})\n",
        "            scheduler.step(val_loss)\n",
        "\n",
        "        # Checkpointing\n",
        "        if (current_step+1) % checkpoint_interval == 0:\n",
        "            torch.save({\n",
        "                'epoch': epoch + 1,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'scheduler_state_dict': scheduler.state_dict(),\n",
        "                'current_step': current_step\n",
        "            }, f\"/content/drive/MyDrive/Colab Notebooks/gpt-mini-background/gpt_mini_4_epoch_{epoch+1}_iter_{i+1}.pth\")\n",
        "            print(f\"Checkpoint saved at epoch {epoch+1} and iteration {i+1}\")\n",
        "\n",
        "        current_step += 1\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{EPOCHS} DONE\")\n",
        "\n",
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "qnwHkfh_5Y-I"
      },
      "outputs": [],
      "source": [
        "load_checkpoint_path = \"gpt_mini_4_epoch_1_iter_90000.pth\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6SOD1boU5Xm-",
        "outputId": "d48fc50f-3fb2-42b8-c5d6-0d519f054b11"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checkpoint loaded from gpt_mini_4_epoch_1_iter_90000.pth\n"
          ]
        }
      ],
      "source": [
        "def load_checkpoint(model, optimizer, filename=load_checkpoint_path):\n",
        "    checkpoint = torch.load(filename)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    print(f\"Checkpoint loaded from {filename}\")\n",
        "\n",
        "load_checkpoint(model, optimizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "TkBysp7L5rLX",
        "outputId": "e02e81cd-06c5-432d-c4e0-6dd270c49255"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.1 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.3/1.1 MB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2023.12.25)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2024.2.2)\n",
            "Installing collected packages: tiktoken\n",
            "Successfully installed tiktoken-0.7.0\n"
          ]
        }
      ],
      "source": [
        "!pip install tiktoken\n",
        "import tiktoken\n",
        "enc = tiktoken.get_encoding(\"gpt2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z_mGD2kY5b9S",
        "outputId": "016505f2-28db-43e8-a696-557530a06c8d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "!\n",
            "\n",
            "POWER SOUTHEY TANK\n",
            "\n",
            "VICE Gov. John Edwards (Key)\n",
            "\n",
            "Updated\n",
            "\n",
            "A source close to the White House said in a statement. He said, \"It was such a big gift I had been buying for years this year.\"\n",
            "\n",
            "A third lieutenant said on after Senator weren't leading an investigation, he said the law will allow agencies to use the software at a minimum of speed.\"\n",
            "\n",
            "Under Rouse's Officer category, he has charged psychopathic generation and falsetications to young men, including it used by a man who first passed a test to the University of Wales and transporting firearm.\n",
            "\n",
            "\"I think we think it's quite fitting for her family in this case,\" she said. \"But if we've been showing very little we do and [further,] we will not have a satisfactory answer.\"\n",
            "\n",
            "CNN's Andrew Smith said she felt \"it was regrettable\" if the cameras became pervasive if she were found under barbers with the cameras, \"I think him is going to own too much for help and will help.\"\n",
            "\n",
            "Median Reading, who\n",
            "\n",
            "Lexington Harmony<|endoftext|>Anxiety Pothal Richards, 59, is vast by the grammatical image of Luke Mille\n",
            "\n",
            "Baha'-J Mill and Popeye readers fear unsuccessfully charged the Argentine series unless they've arrived at a drift. Variety claims that the cult still admitting Christopher Nolan Holmes could be killing innocents after being at Giants' match.\n",
            "\n",
            "Among the subdiurses, Claude Jonas Archec likely had a significant relationship with Ryan Mon therapy, giving an English-language account to become head of Jordan's experiment.\n",
            "\n",
            "The former head of Public Life, written in support of Michael Chandler, \"Baha'-K SH Vada's character, was a Catholic Family, meaning it 79% of its damage.\"\n",
            "\n",
            "No such take by Derrich put the Catholic faith behind James, who won favor his wife Charlie Russell. He continued that the use of James Ludvere Away Syndrome was \"very critical to doing so.\"\n",
            "\n",
            "Paul Gefmah, a Catholic Islamic State Christian and an academic candidate, expressed admiration to Axios that he did not commit, \"He was responsible to reaching in support to Andtersboro. It was the post that they requested.[1]\n",
            "\n",
            "Manress also joined the staff at the State, nightclub, in 1994,, and other groups. He stalks @Amanda sees\n"
          ]
        }
      ],
      "source": [
        "context = torch.zeros((1,1), dtype=torch.long, device=device)\n",
        "print(enc.decode(model.generate(context, max_new_tokens=500)[0].tolist()))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}