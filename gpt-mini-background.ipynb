{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":748,"status":"ok","timestamp":1715756140305,"user":{"displayName":"Yoshi Sato","userId":"03243410627794638618"},"user_tz":-540},"id":"NpcLwGS0x0bq","outputId":"f22b8cbb-efae-4fdc-a01a-38cb2f381b5d"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content/drive/MyDrive/Colab Notebooks/gpt-mini-background\n","'background execution test'     background_task.py\t\t       nohup.out\n","'background execution test 2'   gpt-mini-background-checkpoint-1.pth   prepare.ipynb\n"," background_log.txt\t        gpt-mini-background.ipynb\t       wandb\n"]}],"source":["%cd /content/drive/MyDrive/Colab Notebooks/gpt-mini-background\n","!ls"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":429,"status":"ok","timestamp":1715756186266,"user":{"displayName":"Yoshi Sato","userId":"03243410627794638618"},"user_tz":-540},"id":"usIxm7k0yGNe"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","from torch.nn import functional as F\n","\n","# hyperparameters\n","batch_size = 64 # how many independent sequences will be processed in parallel\n","seq_len = 128 # the maximum context length for predictions\n","vocab_size = 50258\n","learning_rate = 3e-4\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","n_embd = 256 # same as d_model\n","n_head = 8\n","n_layer = 8\n","dropout = 0.2\n","EPOCHS = 20"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":640,"status":"ok","timestamp":1715756188689,"user":{"displayName":"Yoshi Sato","userId":"03243410627794638618"},"user_tz":-540},"id":"cLv-e-e-x64n"},"outputs":[],"source":["import torch\n","from torch.utils.data import Dataset\n","from torch.utils.data import DataLoader\n","import numpy as np\n","\n","class MemmapDataset(Dataset):\n","    def __init__(self, data_dir, seq_len):\n","        self.dataset = np.memmap(data_dir, dtype=np.uint16, mode='r')\n","        self.seq_len = seq_len\n","        self.total_length = len(self.dataset) - seq_len\n","\n","    def __len__(self):\n","        return self.total_length\n","\n","    def __getitem__(self, idx):\n","        input_sequence = self.dataset[idx:idx+self.seq_len].astype(np.int64)\n","        target_sequence = self.dataset[idx+1:idx+self.seq_len+1].astype(np.int64)\n","        return torch.tensor(input_sequence, dtype=torch.long), torch.tensor(target_sequence, dtype=torch.long)\n","\n","\n","train_dir = '/content/drive/MyDrive/Colab Notebooks/Transformers/train10.bin'\n","val_dir = '/content/drive/MyDrive/Colab Notebooks/Transformers/val10.bin'\n","\n","train_dataset = MemmapDataset(train_dir, seq_len=seq_len)\n","val_dataset = MemmapDataset(val_dir, seq_len=seq_len)\n","\n","train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n","val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4)"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1715756190445,"user":{"displayName":"Yoshi Sato","userId":"03243410627794638618"},"user_tz":-540},"id":"UNV1CaTJySYl"},"outputs":[],"source":["class Head(nn.Module):\n","    def __init__(self, head_size):\n","        super().__init__()\n","        self.key = nn.Linear(n_embd, head_size, bias=False)\n","        self.query = nn.Linear(n_embd, head_size, bias=False)\n","        self.value = nn.Linear(n_embd, head_size, bias=False)\n","        self.register_buffer('tril', torch.tril(torch.ones(seq_len, seq_len)))\n","\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, x):\n","        # inputs: (batch, time-step, channels) == (batch_size, seq_len, d_model)\n","        # outputs: (batch, time-step, head size) == (batch_size, seq_len, d_k)\n","        B, T, C = x.shape\n","        k = self.key(x)   # (B, T, hs)\n","        q = self.query(x) # (B, T, hs)\n","\n","        attention_scores = q @ k.transpose(-2, -1) * k.shape[-1]**-0.5 # (B, T, hs) @ (B, hs, T) -> (B, T, T) WAIT HOLD UP THE BIGGEST MISTAKE WAS HERE\n","        attention_scores = attention_scores.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n","        attention_scores = F.softmax(attention_scores, dim=-1)\n","        attention_scores = self.dropout(attention_scores)\n","\n","        v = self.value(x) # (B, T, hs)\n","        out = attention_scores @ v # (B, T, T) @ (B, T, hs) -> (B, T, hs)\n","        return out\n","\n","class MultiHeadAttention(nn.Module):\n","    def __init__(self, num_heads, head_size):\n","        super().__init__()\n","        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n","        self.w_o = nn.Linear(head_size * num_heads, n_embd) # (d_k * h, d_model)\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, x):\n","        out = torch.cat([h(x) for h in self.heads], dim=-1)\n","        out = self.dropout(self.w_o(out))\n","        return out\n","\n","class FeedForward(nn.Module):\n","    def __init__(self, n_embd):\n","        super().__init__()\n","        self.net = nn.Sequential(\n","            nn.Linear(n_embd, 4 * n_embd),\n","            nn.ReLU(),\n","            nn.Linear(4 * n_embd, n_embd),\n","            nn.Dropout(dropout)\n","        )\n","\n","    def forward(self, x):\n","        return self.net(x)\n","\n","class Block(nn.Module):\n","    def __init__(self, n_embd, n_head):\n","        super().__init__()\n","        head_size = n_embd // n_head\n","        self.sa = MultiHeadAttention(n_head, head_size)\n","        self.ffwd = FeedForward(n_embd)\n","        self.ln1 = nn.LayerNorm(n_embd) # planning to make myself\n","        self.ln2 = nn.LayerNorm(n_embd)\n","\n","    def forward(self, x):\n","        x = x + self.sa(self.ln1(x))\n","        x = x + self.ffwd(self.ln2(x))\n","        return x\n","\n","class GPTLanguageModel(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n","        self.position_embedding_table = nn.Embedding(seq_len, n_embd)\n","        self.blocks = nn.Sequential(*[Block(n_embd, n_head) for _ in range(n_layer)])\n","        self.ln_f = nn.LayerNorm(n_embd)\n","        self.lm_head = nn.Linear(n_embd, vocab_size)\n","\n","        self.apply(self._init_weights)\n","\n","    def _init_weights(self, module):\n","        if isinstance(module, nn.Linear):\n","            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n","            if module.bias is not None:\n","                torch.nn.init.zeros_(module.bias)\n","        elif isinstance(module, nn.Embedding):\n","            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n","\n","    def forward(self, idx, targets=None):\n","        B, T = idx.shape\n","\n","        tok_emb = self.token_embedding_table(idx) # (B, T, C) == (Batchsize, seqlen, dmodel)\n","        pos_emb = self.position_embedding_table(torch.arange(T, device=device))\n","        x = tok_emb + pos_emb\n","        x = self.blocks(x) # (B, T, C)\n","        x = self.ln_f(x) # (B, T, C)\n","        logits = self.lm_head(x) # (B, T, vocab_size)\n","\n","        if targets is None:\n","            loss = None\n","        else:\n","            B, T, C = logits.shape\n","            logits = logits.view(B*T, C)\n","            targets = targets.view(B*T)\n","            loss = F.cross_entropy(logits, targets)\n","\n","        return logits, loss\n","\n","    def generate(self, idx, max_new_tokens):\n","        for _ in range(max_new_tokens):\n","            idx_cond = idx[:, -seq_len:]\n","            logits, loss = self(idx_cond)\n","            logits = logits[:, -1, :]\n","            probs = F.softmax(logits, dim=-1)\n","            idx_next = torch.multinomial(probs, num_samples=1)\n","            idx = torch.cat((idx, idx_next), dim=1)\n","        return idx"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6256,"status":"ok","timestamp":1715756198338,"user":{"displayName":"Yoshi Sato","userId":"03243410627794638618"},"user_tz":-540},"id":"9xFQcZ0NyVi1","outputId":"4e0dd440-083a-4048-c4b2-f002d4a6f30e"},"outputs":[{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33myoshisato\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"]},{"data":{"text/plain":["True"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["!pip install wandb -qU\n","import wandb\n","wandb.login(key=\"insert_your_wandb_key_here\")"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":2888,"status":"ok","timestamp":1715756201209,"user":{"displayName":"Yoshi Sato","userId":"03243410627794638618"},"user_tz":-540},"id":"XeL7QOvVyYzE"},"outputs":[],"source":["model = GPTLanguageModel()\n","model = model.to(device)\n","optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":126},"executionInfo":{"elapsed":2422,"status":"ok","timestamp":1715756204207,"user":{"displayName":"Yoshi Sato","userId":"03243410627794638618"},"user_tz":-540},"id":"gw2jo-j-yWs4","outputId":"8e52a3f4-b787-48c1-ef4d-83cf7ee6ddbc"},"outputs":[{"data":{"text/html":["Tracking run with wandb version 0.17.0"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/content/drive/MyDrive/Colab Notebooks/gpt-mini-background/wandb/run-20240515_065641-ngukem6a</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href='https://wandb.ai/yoshisato/gpt-mini-training-project/runs/ngukem6a' target=\"_blank\">drawn-frog-3</a></strong> to <a href='https://wandb.ai/yoshisato/gpt-mini-training-project' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View project at <a href='https://wandb.ai/yoshisato/gpt-mini-training-project' target=\"_blank\">https://wandb.ai/yoshisato/gpt-mini-training-project</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run at <a href='https://wandb.ai/yoshisato/gpt-mini-training-project/runs/ngukem6a' target=\"_blank\">https://wandb.ai/yoshisato/gpt-mini-training-project/runs/ngukem6a</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/yoshisato/gpt-mini-training-project/runs/ngukem6a?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"],"text/plain":["<wandb.sdk.wandb_run.Run at 0x797071ffbaf0>"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["wandb.init(\n","    project=\"gpt-mini-training-project\",\n","\n","    config={\n","    \"learning_rate\": 0.0003,\n","    \"architecture\": \"Decoder-only Transformer\",\n","    \"dataset\": \"OpenWebText\",\n","    \"EPOCHS\": EPOCHS,\n","    \"Iterations per Epoch\": len(train_dataloader),\n","    \"Model Parameters\" : sum(p.numel() for p in model.parameters() if p.requires_grad),\n","    }\n",")"]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":429,"status":"ok","timestamp":1715756229264,"user":{"displayName":"Yoshi Sato","userId":"03243410627794638618"},"user_tz":-540},"id":"32gi1spxyi1o"},"outputs":[],"source":["@torch.no_grad()\n","def estimate_loss():\n","    model.eval()\n","    running_loss = 0.0;\n","    for batch in val_dataloader:\n","        val_input_sequences, val_target_sequences = batch\n","        val_input_sequences = val_input_sequences.to(device)\n","        val_target_sequences = val_target_sequences.to(device)\n","\n","        logits, loss = model(val_input_sequences, val_target_sequences)\n","        running_loss += loss.item()\n","\n","    avg_loss = running_loss / len(val_dataloader)\n","    model.train()\n","    return avg_loss"]},{"cell_type":"code","execution_count":14,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1715756238098,"user":{"displayName":"Yoshi Sato","userId":"03243410627794638618"},"user_tz":-540},"id":"iCCbNRf-5KbH"},"outputs":[],"source":["def save_checkpoint(model, optimizer, filename):\n","    checkpoint = {\n","        'model_state_dict': model.state_dict(),\n","        'optimizer_state_dict': optimizer.state_dict()\n","    }\n","    torch.save(checkpoint, filename)\n","    print(f\"Checkpoint saved to {filename}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"awiM2_7Gydmx","outputId":"97df771b-48dc-49a0-d86e-2b16dc128c7c"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: 1/20, Batch: 2000/176615, Loss: 5.953968112945557\n","Epoch: 1/20, Batch: 4000/176615, Loss: 5.036702615976334\n","Epoch: 1/20, Batch: 6000/176615, Loss: 4.691449282169342\n","Epoch: 1/20, Batch: 8000/176615, Loss: 4.491022644758225\n","Epoch: 1/20, Batch: 10000/176615, Loss: 4.341094835877419\n","Epoch: 1/20, Batch: 12000/176615, Loss: 4.231838054418564\n","Epoch: 1/20, Batch: 14000/176615, Loss: 4.1418916771411896\n","Epoch: 1/20, Batch: 16000/176615, Loss: 4.0663975385427475\n","Epoch: 1/20, Batch: 18000/176615, Loss: 4.004793575406074\n","Epoch: 1/20, Batch: 20000/176615, Loss: 3.9494855947494507\n","Checkpoint saved to /content/drive/MyDrive/Colab Notebooks/gpt-mini-background/gpt_mini_epoch_1_iter_20000.pth\n","Epoch: 1/20, Batch: 20000/176615, Validation Loss: 5.151443438518905\n","Epoch: 1/20, Batch: 22000/176615, Loss: 3.900879245042801\n","Epoch: 1/20, Batch: 24000/176615, Loss: 3.86182288646698\n","Epoch: 1/20, Batch: 26000/176615, Loss: 3.8236351563930513\n","Epoch: 1/20, Batch: 28000/176615, Loss: 3.7922455523014067\n","Epoch: 1/20, Batch: 30000/176615, Loss: 3.763895741581917\n","Epoch: 1/20, Batch: 32000/176615, Loss: 3.735713050365448\n","Epoch: 1/20, Batch: 34000/176615, Loss: 3.714185962319374\n","Epoch: 1/20, Batch: 36000/176615, Loss: 3.6884419951438905\n"]}],"source":["total_batches = len(train_dataloader) # len(train) = 176,615, len(val) = 2155\n","log_interval = 2000\n","val_interval = 20000\n","\n","for epoch in range(EPOCHS):\n","    running_loss = 0.0\n","\n","    for i, batch in enumerate(train_dataloader):\n","        input_sequences, target_sequences = batch\n","        input_sequences = input_sequences.to(device)\n","        target_sequences = target_sequences.to(device)\n","\n","        logits, loss = model(input_sequences, target_sequences)\n","        optimizer.zero_grad(set_to_none=True)\n","        loss.backward()\n","        optimizer.step()\n","        running_loss += loss.item()\n","\n","        if (i+1) % log_interval == 0:\n","            avg_loss = running_loss / log_interval\n","            print(f\"Epoch: {epoch+1}/{EPOCHS}, Batch: {i+1}/{total_batches}, Loss: {avg_loss}\")\n","            wandb.log({\"Training Loss\": avg_loss})\n","            running_loss = 0.0\n","\n","        if (i+1) % val_interval == 0:\n","            save_checkpoint(model, optimizer, f\"/content/drive/MyDrive/Colab Notebooks/gpt-mini-background/gpt_mini_epoch_{epoch+1}_iter_{i+1}.pth\")\n","            val_loss = estimate_loss()\n","            print(f\"Epoch: {epoch+1}/{EPOCHS}, Batch: {i+1}/{total_batches}, Validation Loss: {val_loss}\")\n","            wandb.log({\"Validation Loss\": val_loss})\n","\n","    print(f\"Epoch {epoch+1}/{EPOCHS} DONE\")\n","    wandb.log({\"Epoch\": epoch+1})\n","    save_checkpoint(model, optimizer, f\"/content/drive/MyDrive/Colab Notebooks/gpt-mini-background/gpt_mini_epoch_{epoch+1}.pth\")\n","\n","wandb.finish()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qnwHkfh_5Y-I"},"outputs":[],"source":["load_checkpoint_path = \"gpt-mini-background-checkpoint-1.pth\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6SOD1boU5Xm-"},"outputs":[],"source":["def load_checkpoint(model, optimizer, filename=load_checkpoint_path):\n","    checkpoint = torch.load(filename)\n","    model.load_state_dict(checkpoint['model_state_dict'])\n","    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n","    print(f\"Checkpoint loaded from {filename}\")\n","\n","load_checkpoint(model, optimizer)"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21802,"status":"ok","timestamp":1715753881963,"user":{"displayName":"Yoshi Sato","userId":"03243410627794638618"},"user_tz":-540},"id":"Z_mGD2kY5b9S","outputId":"bbd09b29-64fc-41ae-e8d2-f3c107d14784"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting tiktoken\n","  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2023.12.25)\n","Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.31.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2024.2.2)\n","Installing collected packages: tiktoken\n","Successfully installed tiktoken-0.7.0\n","!”\n","\n","The papers mentioned above the σ, with certain v3 k.m. . The sediment “contreactory machinery” were “d.‐esulate S –-dependent,” its statistic was further pronounced.\n","\n","One paper:\n","\n","“Once the operation was hailed by fossil and “other seconds, the production transports/slank to the reactor fragments, C and R.A. Assolyl (HA) to the kW of the liquid grains, so the insertion of particles down close to the BP magnitude longer and data.” It does not work closely with graph Data. Broad Self-confirmed suggests that industry is recently driven by various solar industries (00 percent increase in production).\n","\n","It’s all that James Costle seeks in this emerging economy of reactor projects, abuses which are very similar to conventional crude-renances, and that we are actually going to come up quickly. Everything is used for repeals. It’s 20 percent of this spike has torn into Hirds and silicon mens for 2017, as accounts for the overall increase about 5 million years.\n","\n","Other\n","\n","Inflation, the investment in the Ford sector threatens the health boom and instability.\n","\n","The period has expired at Amazon’s Open Trade Center. In 2008, Hudson, a construction collectors that produce nuclear, and also the auto industry, producing component technologies, is covered with wind output that has led from Juno to the edge very slow rule and its continual popularity. In addition, he turns it from a coal plant and plant with what he’s done from the 1970s.\n","\n","HOST4 remembers that oil production has been ideal.\n","\n","“In terms of business, and in the U.S. economy economy under growth it is hard to teach,” Sherman says. “They don’t think this would be a meritocracy.\n","\n","Oody’Sweeney is “actively without games but not yet; are the ones receiving a clear indication that the energy ownership website has produced in Europe.”\n","\n","While the previous report, Martin Pollent has indicated it should be better.\n","\n","“Baltimore Farmers are well-designed on the basis of demographic change, even than people who have the chance to join any technical forum that has a budget of about 60 times: less labor and social support, elevating employment, politics\n"]}],"source":["!pip install tiktoken\n","import tiktoken\n","enc = tiktoken.get_encoding(\"gpt2\")\n","context = torch.zeros((1,1), dtype=torch.long, device=device)\n","print(enc.decode(model.generate(context, max_new_tokens=500)[0].tolist()))"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyMaEEV/Im2elCEDTuAdZLyx","gpuType":"A100","machine_shape":"hm","mount_file_id":"1YfO4DkScxH83yVJYeRGpc9hZ2bLSkISK","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
